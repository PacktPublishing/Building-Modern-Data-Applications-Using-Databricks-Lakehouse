# Building Modern Data Applications using the Databricks Lakehouse

**Published by Packt**

## Description
With so much time spent on tooling in today’s data engineering development stack, operational complexity takes over as data engineers spend less time gleaning value from their data and more time maintaining complex data pipelines. 
Delta Live Tables framework aims to remove this complexity by simplifying the data pipeline development to just defining input data sources, transformation logic, and output table destinations.  

 
## Why should you read this book
In this book, you’ll start with an overview of the Delta Lake format, the Databricks platform, and the Delta Live Tables framework. 
Next, you’ll dive into applying data transformations by implementing the Databricks medallion architecture and continuously monitoring the data quality of your pipelines. 
You’ll learn how to react to incoming data using the Databricks Auto Loader feature and automate real-time data processing using Databricks workflows. 
We’ll learn how to recover from runtime errors automatically, repair failed workflow runs, and backfill tables when table changes are necessary. 

By the end of this book, you will have mastered building a real-time data pipeline from scratch using Delta Live Tables. You’ll also have learned to use CI/CD tools to deploy data pipeline changes automatically across deployment environments and monitor, control, and optimize cloud costs. 


## What will you learn 

- Quickly deploy real-time data pipelines on Databricks using Auto Loader and Delta Live Tables. 
- Explore advanced features of the Delta Lake format, like schema evolution, time travel, and liquid clustering. 
- Orchestrate multiple data pipelines using Databricks Workflows. 
- Implement data validation policies to quarantine invalid data as soon as it arrives in the Lakehouse. 
- Secure data access across various users and groups using access policies in Unity Catalog. 
- Automate the continuous deployment of data pipeline changes by integrating a Git provider with Databricks Repos. 


## Who should read this book 
This book is for data engineers, data scientists, and other data stewards managing enterprise data processing for their organizations. 
This book will simplify learning advanced data engineering techniques on Databricks, making implementing a cutting-edge Lakehouse accessible to individuals with varying technical expertise. 
However, a beginner-level knowledge of Apache Spark and Python is needed to make the most out of the code samples contained within this book. 


## About the Author 
Will Girten is a Lead Specialist Solutions Architect at Databricks who joined Databricks in early 2019. 
With over a decade of experience in data and AI, Will has worked in various business verticals, from healthcare to government and financial services. 
Will’s primary focus has been helping enterprises implement data warehousing strategies for the Lakehouse and performance-tuning BI dashboards, reports, and queries. Will is a certified Databricks Data Engineering Professional and Databricks Machine Learning Professional. 
He holds a Bachelor of Science in Computer Engineering from the University of Delaware. 
